{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04f38ca0",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "\n",
    "This benchmark is based on [this article](https://www.depends-on-the-definition.com/identify-ingredients-with-neural-networks/) with some minor adaptations for the problem we are trying to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49a5494a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (21.0.1)\n",
      "Collecting pip\n",
      "  Downloading pip-21.1.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 16.3 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.0.1\n",
      "    Uninstalling pip-21.0.1:\n",
      "      Successfully uninstalled pip-21.0.1\n",
      "Successfully installed pip-21.1.2\n",
      "Collecting spacy==2.3.5\n",
      "  Downloading spacy-2.3.5-cp36-cp36m-manylinux2014_x86_64.whl (10.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.4 MB 17.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.5-cp36-cp36m-manylinux2014_x86_64.whl (35 kB)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy==2.3.5) (49.6.0.post20210108)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy==2.3.5) (1.19.5)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.5-cp36-cp36m-manylinux2014_x86_64.whl (20 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.5-cp36-cp36m-manylinux2014_x86_64.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 72.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting srsly<1.1.0,>=1.0.2\n",
      "  Downloading srsly-1.0.5-cp36-cp36m-manylinux2014_x86_64.whl (184 kB)\n",
      "\u001b[K     |████████████████████████████████| 184 kB 71.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy==2.3.5) (2.25.1)\n",
      "Collecting tqdm<5.0.0,>=4.38.0\n",
      "  Downloading tqdm-4.61.0-py2.py3-none-any.whl (75 kB)\n",
      "\u001b[K     |████████████████████████████████| 75 kB 6.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting plac<1.2.0,>=0.9.6\n",
      "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.4-cp36-cp36m-manylinux2014_x86_64.whl (9.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.8 MB 64.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7\n",
      "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Collecting wasabi<1.1.0,>=0.4.0\n",
      "  Downloading wasabi-0.8.2-py3-none-any.whl (23 kB)\n",
      "Collecting thinc<7.5.0,>=7.4.1\n",
      "  Downloading thinc-7.4.5-cp36-cp36m-manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 56.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata>=0.20 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.3.5) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.3.5) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.3.5) (3.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (3.0.4)\n",
      "Installing collected packages: murmurhash, cymem, wasabi, tqdm, srsly, preshed, plac, catalogue, blis, thinc, spacy\n",
      "Successfully installed blis-0.7.4 catalogue-1.0.0 cymem-2.0.5 murmurhash-1.0.5 plac-1.1.3 preshed-3.0.5 spacy-2.3.5 srsly-1.0.5 thinc-7.4.5 tqdm-4.61.0 wasabi-0.8.2\n",
      "Collecting de_core_news_sm==2.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.3.0/de_core_news_sm-2.3.0.tar.gz (14.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.9 MB 6.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from de_core_news_sm==2.3.0) (2.3.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.19.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (0.7.4)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.0)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (7.4.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (0.8.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.25.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.1.3)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (49.6.0.post20210108)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (4.61.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.5)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.7.4.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.10)\n",
      "Building wheels for collected packages: de-core-news-sm\n",
      "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.3.0-py3-none-any.whl size=14907581 sha256=d3f14d85ea2f878e44a81a64d2d3b3d0eb116da4d81d31e19f3272bd41d2e7a0\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-5lpvw1sb/wheels/fe/44/0f/7270b8ec13bc290e606a3c0f52f981915b1d09d1dfc7c79088\n",
      "Successfully built de-core-news-sm\n",
      "Installing collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-2.3.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('de_core_news_sm')\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.5.0-cp36-cp36m-manylinux2010_x86_64.whl (454.3 MB)\n",
      "\u001b[K     |█████████████████████           | 298.2 MB 76.0 MB/s eta 0:00:03    |████▌                           | 63.3 MB 17.4 MB/s eta 0:00:23     |██████                          | 84.9 MB 71.3 MB/s eta 0:00:06     |███████████████▉                | 225.5 MB 66.9 MB/s eta 0:00:04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 454.3 MB 11 kB/s s eta 0:00:01     |████████████████████████████▏   | 400.0 MB 69.5 MB/s eta 0:00:01     |██████████████████████████████▊ | 436.2 MB 70.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.36.2)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.1.0)\n",
      "Collecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
      "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 50.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 5.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.15.2)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting keras-nightly~=2.5.0.dev\n",
      "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 52.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy~=1.19.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.19.5)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wrapt~=1.12.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.12.1)\n",
      "Collecting tensorboard~=2.5\n",
      "  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 52.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions~=3.7.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.7.4.3)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 77.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio~=1.34.0\n",
      "  Downloading grpcio-1.34.1-cp36-cp36m-manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 47.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six~=1.15.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.2.0)\n",
      "Collecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: cached-property in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from h5py~=3.1.0->tensorflow) (1.5.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard~=2.5->tensorflow) (2.25.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 48.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 10.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard~=2.5->tensorflow) (1.0.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard~=2.5->tensorflow) (49.6.0.post20210108)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 50.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.30.1-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 78.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 76.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (3.0.4)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 72.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow) (3.4.0)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=7974a8bfca20af2bc7978e913c4ca0ceff9ff3e88ae7f749415542bbb5a43b4b\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "Successfully built termcolor\n",
      "Installing collected packages: pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, keras-nightly, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed absl-py-0.12.0 astunparse-1.6.3 cachetools-4.2.2 flatbuffers-1.12 gast-0.4.0 google-auth-1.30.1 google-auth-oauthlib-0.4.4 grpcio-1.34.1 keras-nightly-2.5.0.dev2021032900 keras-preprocessing-1.1.2 markdown-3.3.4 oauthlib-3.1.1 opt-einsum-3.3.0 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.5.0 tensorflow-estimator-2.5.0 termcolor-1.1.0\n",
      "Collecting keras\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.5.3)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.19.5)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (3.1.0)\n",
      "Requirement already satisfied: cached-property in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from h5py->keras) (1.5.1)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.4.3\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    already_initialized\n",
    "except NameError:\n",
    "    !python -m pip install --upgrade pip\n",
    "    !pip install spacy==2.3.5\n",
    "    !python -m spacy download de_core_news_sm\n",
    "    !pip install tensorflow\n",
    "    !pip install keras\n",
    "    already_initialized = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27e7d253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n",
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "783e34ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Die Kirschen abtropfen lassen, dabei den Saft auffangen. Das Puddingpulver mit dem Vanillezucker mischen und mit 6 EL Saft glatt rühren. Den übrigen Kirschsaft aufkochen und vom Herd nehmen. Das angerührte Puddingpulver einrühren und unter Rühren ca. eine Minute köcheln. Die Kirschen unter den angedickten Saft geben. Milch, 40 g Zucker, Vanillemark und Butter aufkochen. Den Topf vom Herd ziehen und den Grieß unter Rühren einstreuen. Unter Rühren einmal aufkochen lassen und zugedeckt ca. fünf Minuten quellen lassen.In der Zeit das Ei trennen. Das Eiweiß mit einer Prise Salz steif schlagen und dabei die restlichen 20 g Zucker einrieseln lassen. Das Eigelb unter den Brei rühren und dann das Eiweiß unterheben.Den Grießbrei mit dem Kompott servieren.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(\"data/recipes.json\")\n",
    "df.Instructions[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e70ed52b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1190, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df = df[11000:]\n",
    "eval_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35155e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11000, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[:11000]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dc46df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 17556\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('de_core_news_sm', disable=['parser', 'tagger', 'ner'])\n",
    "tokenized = [nlp(t) for t in df.Instructions.values]\n",
    "vocab = {\"<UNK>\": 1, \"<PAD>\": 0}\n",
    "for txt in tokenized:\n",
    "    for token in txt:\n",
    "        if token.text not in vocab.keys():\n",
    "            vocab[token.text] = len(vocab)\n",
    "print(\"Number of unique tokens: {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac9c5f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['600 g Hackfleisch, halb und halb',\n",
       " '800 g Sauerkraut',\n",
       " '200 g Wurst, geräucherte (Csabai Kolbász)',\n",
       " '150 g Speck, durchwachsener, geräucherter',\n",
       " '100 g Reis',\n",
       " '1 m.-große Zwiebel(n)',\n",
       " '1 Zehe/n Knoblauch',\n",
       " '2 Becher Schmand',\n",
       " '1/2TL Kümmel, ganzer',\n",
       " '2 Lorbeerblätter',\n",
       " 'Salz und Pfeffer',\n",
       " '4 Ei(er) (bei Bedarf)',\n",
       " 'Paprikapulver',\n",
       " 'etwas Wasser',\n",
       " 'Öl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredients = df.Ingredients\n",
    "ingredients[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1297c71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _filter(token):\n",
    "    if len(token) < 2:\n",
    "        return False\n",
    "    if token.is_stop:\n",
    "        return False\n",
    "    if token.text[0].islower():\n",
    "        return False\n",
    "    if token.is_digit:\n",
    "        return False\n",
    "    if token.like_num:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def _clean(text):\n",
    "    text = text.replace(\"(\", \"\")\n",
    "    text = text.split(\"/\")[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08e211ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rosenkohl',\n",
       " 'Schalotte',\n",
       " '2Tasse',\n",
       " 'Hühnerbrühe',\n",
       " 'Milch',\n",
       " 'EL',\n",
       " 'Crème',\n",
       " 'Speck',\n",
       " 'Kartoffelgnocchi']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean = [_clean(t.text) for i in ingredients[214] for t in nlp(i) if _filter(t) and len(_clean(t.text)) >= 2]\n",
    "clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2860e642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(ingredients, tokenized_instructions):\n",
    "    labels = []\n",
    "    for ing, ti in zip(ingredients, tokenized_instructions):\n",
    "        l_i = []\n",
    "        ci = [_clean(t.text) for i in ing for t in nlp(i) if _filter(t) and len(_clean(t.text)) >= 2]\n",
    "        label = []\n",
    "        for token in ti:\n",
    "            l_i.append(any((c == token.text or c == token.text[:-1] or c[:-1] == token.text) for c in ci))\n",
    "        labels.append(l_i)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d9520a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Crème', 'Hühnerbrühe', 'Milch', 'Rosenkohl', 'Schalotten', 'Speck'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = get_labels(ingredients, tokenized)\n",
    "set([t.text for t, l in zip(tokenized[214], labels[214]) if l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec302f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.hist([len([t for t in tokens]) for tokens in tokenized], bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "563c1f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62884671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(texts, max_len, vocab={\"<UNK>\": 1, \"<PAD>\": 0}):\n",
    "    X = [[vocab.get(w.text, vocab[\"<UNK>\"]) for w in s] for s in texts]\n",
    "    return pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=vocab[\"<PAD>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "085f162d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([192, 193, 194, 183, 195, 196, 128, 197,   9, 198, 199, 200, 201,\n",
       "       202, 203,  60, 204, 205,   9,  13, 206,  15,  23,  98, 207, 208,\n",
       "        51, 209,  68, 202, 203,  25,   6, 195, 125, 202, 210, 211, 212,\n",
       "        33,  45, 213, 214, 100, 196,  13, 215, 216, 217,  33,   9,  68,\n",
       "       218, 219, 213, 169,  35,  82, 100, 220, 221, 202,   6, 222,  45,\n",
       "       223,  48, 224,  33,  67, 225, 100, 226,   6, 227, 228, 229, 130,\n",
       "        45,  92,  85, 230, 211, 231,   6, 232, 233, 234, 235, 145, 157,\n",
       "       236,   9, 237, 238, 104, 239, 210, 240, 157, 241,  54,   6, 109,\n",
       "       242, 243, 244, 245, 246, 187, 247,   6, 248, 183, 249, 250,  33,\n",
       "       129,  13, 251, 252, 101, 253,  33, 254,   9,  31, 255,  40, 172,\n",
       "         6,   2, 256, 257, 177, 258, 259, 260,  33,  42, 261, 262, 263,\n",
       "       131, 264, 265, 266,  33, 267,  74, 268, 269,  68, 270,   6,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_seq = prepare_sequences(tokenized, max_len=MAX_LEN, vocab=vocab)\n",
    "X_seq[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f55f610",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_seq = []\n",
    "for l in labels:\n",
    "    y_i = []\n",
    "    for i in range(MAX_LEN):\n",
    "        try:\n",
    "            y_i.append(float(l[i]))\n",
    "        except:\n",
    "            y_i.append(0.0)\n",
    "    y_seq.append(np.array(y_i))\n",
    "y_seq = np.array(y_seq)\n",
    "y_seq = y_seq.reshape(y_seq.shape[0], y_seq.shape[1], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffa2e67",
   "metadata": {},
   "source": [
    "# Setup the network"
   ]
  },
  {
   "cell_type": "raw",
   "id": "54e6c6fa",
   "metadata": {},
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Embedding(input_dim=len(vocab), mask_zero=True, output_dim=50))\n",
    "model.add(layers.SpatialDropout1D(0.2))\n",
    "model.add(layers.Bidirectional(layers.LSTM(units=64, return_sequences=True)))\n",
    "model.add(layers.SpatialDropout1D(0.2))\n",
    "model.add(layers.Bidirectional(layers.LSTM(units=64, return_sequences=True)))\n",
    "model.add(layers.TimeDistributed(layers.Dense(1, activation='sigmoid')))\n",
    "\n",
    "model.compile(optimizer=tf.optimizers.Adam(0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f2c9bbb",
   "metadata": {},
   "source": [
    "history = model.fit(X_seq, y_seq, epochs=10, batch_size=256, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fc0ba8bc",
   "metadata": {},
   "source": [
    "from datetime import datetime\n",
    "now = datetime.today().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "model_name = f'benchmark_model_{now}_loss-{history.history[\"loss\"][-1]:.2f}'\n",
    "model.save(f'./model/{model_name}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6eebc72a",
   "metadata": {},
   "source": [
    "from s3util import S3Util\n",
    "s3util = S3Util()\n",
    "s3util.save_models_to_s3()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6108106",
   "metadata": {},
   "source": [
    "from importlib import reload\n",
    "import s3util\n",
    "reload(s3util)\n",
    "model_name = 'benchmark_model_2021-03-13_14-57-29_loss-0.02'\n",
    "s3u = s3util.S3Util()\n",
    "s3u.get_model_from_s3(model_name)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "402cdf25",
   "metadata": {},
   "source": [
    "s3u.unpack(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9faa4786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('model/benchmark_model_2021-03-13_14-57-29_loss-0.02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02c8999a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 25s 936ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_seq, verbose=1, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b5f0efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3343\n",
    "pred_i = y_pred[i] > 0.05 # Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b80d9417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Kohlrabi schälen, waschen und in Stifte schneiden. Brühe und Milch ankochen, Kohlrabi dazugeben, aufkochen lassen und 10 Minuten kochen. Dann herausnehmen und abtropfen lassen, die Brühe aufheben.Butter erhitzen, das Mehl darin anschwitzen, mit Kohlrabibrühe ablöschen und aufkochen lassen. Mit den Gewürzen abschmecken. Kohlrabi wieder dazugeben.Hähnchenbrust schnetzeln, kräftig anbraten und würzen. Das Fleisch in eine Auflaufform geben, die Speckwürfel darüber verteilen. Mit Käse bestreuen. Nun das Gemüse darüber schichten und alles bei 180 °C ca. 25 Minuten überbacken.Tipp:Man kann auch gut gekochte, in Würfel geschnittene Kartoffeln unter die Kohlrabi mischen. Ebenso kann man auch Kohlrabi und Möhren für den Auflauf nehmen. Schmeckt auch sehr lecker!"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[i] # Original text as a list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e27c8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Brühe', 'Gemüse', 'Kartoffeln', 'Kohlrabi', 'Butter', 'Speckwürfel', 'Käse', 'Möhren', 'Milch', 'Mehl'}\n"
     ]
    }
   ],
   "source": [
    "ingreds = [t.text for t, p in zip(tokenized[i], pred_i) if p] # The list of tokens in the text with a prediction greater than 0.05\n",
    "print(set(ingreds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba8be912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Butter', 'Kohlrabi', 'Käse', 'Mehl', 'Milch'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingreds = [t.text for t, p in zip(tokenized[i], y_seq[i]) if p] # The list of tokens in the text that were originally labeled as ingredients\n",
    "set(ingreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fc21b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['500 g Kohlrabi',\n",
       " '1/4Liter Hühnerbrühe',\n",
       " '1/4Liter Milch',\n",
       " '1 EL Butter',\n",
       " '30 g Mehl',\n",
       " '300 g Hähnchenbrustfilet(s)',\n",
       " 'Salz und Pfeffer',\n",
       " 'Muskat',\n",
       " '50 g Käse, gerieben',\n",
       " '50 g Speck, gewürfelt']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredients[i] # The actual list of ingredients used for labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd50f543",
   "metadata": {},
   "source": [
    "This looks very good! Our model seems to be able to identify the ingredients better than our training labels. So we now use the produced labels for fine-tuning the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "703adf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = []\n",
    "for pred_i, ti in zip(y_pred, tokenized):\n",
    "    l_i = []\n",
    "    ci = [t.text for t, p in zip(tokenized[i], pred_i > 0.05) if p]\n",
    "    label = []\n",
    "    for token in ti:\n",
    "        l_i.append(any((c == token.text or c == token.text[:-1] or c[:-1] == token.text) for c in ci))\n",
    "    new_labels.append(l_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8502943",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_seq_new = []\n",
    "for l in new_labels:\n",
    "    y_i = []\n",
    "    for i in range(MAX_LEN):\n",
    "        try:\n",
    "            y_i.append(float(l[i]))\n",
    "        except:\n",
    "            y_i.append(0.0)\n",
    "    y_seq_new.append(np.array(y_i))\n",
    "y_seq_new = np.array(y_seq_new)\n",
    "y_seq_new = y_seq.reshape(y_seq_new.shape[0], y_seq_new.shape[1], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a953a7",
   "metadata": {},
   "source": [
    "We fit the network again for one epoch with the new labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4294464b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 149s 4s/step - loss: 0.0191 - accuracy: 0.9805 - val_loss: 0.0203 - val_accuracy: 0.9802\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_seq, y_seq_new, epochs=1, batch_size=256, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e222d0",
   "metadata": {},
   "source": [
    "# Look at test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4dd846fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['500 g Rosenkohl',\n",
       " '400 g Kartoffel(n)',\n",
       " '3 Stange/n Staudensellerie',\n",
       " '2 Zwiebel(n), gewürfelte',\n",
       " '4 EL Olivenöl',\n",
       " '1 TL Currypulver',\n",
       " '1/2TL Kurkuma',\n",
       " '300 ml Gemüsebrühe',\n",
       " '3 EL Tomatenmark',\n",
       " '50 g Rosinen',\n",
       " '1/2TL Chilipulver oder frischer Chili',\n",
       " '1/2Bund Petersilie',\n",
       " '2 Banane(n)',\n",
       " '1/2 Zitrone(n), der Saft davon',\n",
       " '150 g Schlagsahne',\n",
       " '1 EL Zucker, braun bei Bedarf']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_ingredients = eval_df.Ingredients.values\n",
    "eval_ingredients[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d28ffa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rosenkohl putzen, die Röschen halbieren. Kartoffeln schälen und würfeln. Staudensellerie putzen und in 1 cm große Stücke schneiden. Zwiebeln schälen und würfeln.Öl in einer Pfanne erhitzen, Chili und Zwiebeln darin anbraten. Mit Currypulver und Kurkuma bestreuen. Tomatenmark zugeben und kurz mit anschwitzen (je nach Geschmack 1 EL braunen Zucker zugeben). Rosenkohl, Kartoffeln und Sellerie zugeben. Kurz andünsten und die Brühe angießen. Rosinen zugeben und mit geschlossenem Deckel ca. 15 Minuten köcheln.Die Bananen schälen und in Scheiben schneiden. Die Bananenscheiben und Zitronensaft zugeben und noch einmal 10 Minuten köcheln lassen. Die Sahne nicht ganz steif schlagen und unter das Rosenkohl-Curry heben. Mit Petersilie bestreut servieren."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_tokenized = [nlp(t) for t in eval_df.Instructions.values]\n",
    "eval_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "935ffbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3285,  205,   33,   98,  564,  869,    6,  520,   32,    9,   30,\n",
       "          6, 3669,  205,    9,   13,  110,  333,  264,  206,   15,    6,\n",
       "        585,   32,    9,   30,    6,   92,   13,  233,  420,  459,   33,\n",
       "       2271,    9,  585,  422,  341,    6,  109, 1236,    9, 1680,  548,\n",
       "          6,  612,  568,    9,   62,   10,  703,   23,  361,  199,  200,\n",
       "        110,  277, 3670,  295,  568,   25,    6, 3285,   33,  520,    9,\n",
       "        560,  568,    6,  691, 1132,    9,   98,  220,  570,    6, 1725,\n",
       "        568,    9,   10, 1254,  148,  288,  571,   79,  290,    6,    2,\n",
       "       3353,   32,    9,   13,   14,   15,    6,    2, 3671,    9, 1122,\n",
       "        568,    9,  124,  302,  344,   79,  290,   54,    6,    2,  426,\n",
       "         51,   52,  311,  312,    9,   80,   45, 3672, 1083,    6,  109,\n",
       "        380,  574,  321,    6,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_seq_test = prepare_sequences(eval_tokenized, max_len=MAX_LEN, vocab=vocab)\n",
    "X_seq_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d077c087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 340ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = model.predict(X_seq_test, verbose=1, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47110adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(i):\n",
    "    pred_i = y_pred_test[i] > 0.05\n",
    "    print(eval_tokenized[i])\n",
    "    print()\n",
    "    print(eval_ingredients[i])\n",
    "    print()\n",
    "    ingreds = [t.text for t, p in zip(eval_tokenized[i], pred_i) if p]\n",
    "    print(set(ingreds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c7903e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Den Quark durch ein Sieb in eine tiefe Schüssel streichen.Das Mehl, den Zucker, Salz, Vanillezucker und das rohe Ei/er gut verrühren.Diese Masse auf einem mit Mehl bestreuten Backbrett zu einer dicken Wurst rollen und in 10 gleichgroße Scheiben schneiden. In heißer Butter von beiden Seiten goldbraun braten.Die fertigen Tworoshniki werden mit Puderzucker bestreut oder warm mit saurer Sahne oder Obstsirup zu Tisch gebracht.\n",
      "\n",
      "['500 g Quark, sehr trockenen', '80 g Mehl', '2 EL Zucker', '1 Pck. Vanillezucker', 'Salz', '1 Ei(er), evt. 2', '4 EL Butter oder Margarine', 'Puderzucker', '125 ml Sirup (Obstsirup) oder saure Sahne', 'Mehl für die Arbeitsfläche']\n",
      "\n",
      "{'Puderzucker', 'Wurst', 'Zucker', 'Butter', 'Vanillezucker', 'Sahne', 'Ei', 'Quark', 'Obstsirup', 'Salz', 'Mehl'}\n"
     ]
    }
   ],
   "source": [
    "pred(893)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "51169f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spargel putzen und bissfest garen. Herausnehmen, abschrecken und warm stellen.Fisch mit Salz und Pfeffer würzen. Öl in einer Pfanne erhitzen und den Lachs darin 3-4 Min. je Seite braten. Butter schmelzen, Mandeln hinzufügen und leicht bräunen. Schale der Limette mit einem Zestenreißer abziehen, den Saft auspressen, beides in die Butter geben. Mit Salz und Pfeffer würzen.Spargel abtropfen lassen, mit Lachs anrichten und mit Mandelbutter beträufeln.Dazu passen Salzkartoffeln.\n",
      "\n",
      "['500 g Spargel, weißer', '500 g Spargel, grüner', 'Salz und Pfeffer', '4 Scheibe/n Lachsfilet(s) (à ca. 200g)', '2 EL Öl', '100 g Butter', '30 g Mandel(n) in Blättchen', '1 Limette(n), unbehandelt']\n",
      "\n",
      "{'Pfeffer', 'Öl', 'Saft', 'Lachs', 'Butter', 'Limette', 'Mandeln', 'Fisch', 'Spargel', 'Salz'}\n"
     ]
    }
   ],
   "source": [
    "pred(26)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
