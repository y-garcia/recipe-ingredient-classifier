{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c9cd768",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "\n",
    "This benchmark is based on [this article](https://www.depends-on-the-definition.com/identify-ingredients-with-neural-networks/) with some minor adaptations for the problem we are trying to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a52018",
   "metadata": {},
   "source": [
    "## Learn to identify ingredients with neural networks\n",
    "\n",
    "**Author**: Today we want to build a model, that can identify ingredients in cooking recipes. I use the [German Recipes Dataset](https://www.kaggle.com/sterby/german-recipes-dataset), I recently published on kaggle. We have more than 12000 German recipes and their ingredients list. First we will generate labels for every word in the recipe, if it is an ingredient or not. Then we use a sequence-to-sequence neural network to tag every word. Then we pseudo-label the training set and update the model with the new labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8576b0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (21.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-21.1.3-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 22.9 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.1.2\n",
      "    Uninstalling pip-21.1.2:\n",
      "      Successfully uninstalled pip-21.1.2\n",
      "Successfully installed pip-21.1.3\n",
      "Collecting spacy==2.3.5\n",
      "  Downloading spacy-2.3.5-cp36-cp36m-manylinux2014_x86_64.whl (10.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.4 MB 11.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting thinc<7.5.0,>=7.4.1\n",
      "  Downloading thinc-7.4.5-cp36-cp36m-manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 61.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy==2.3.5) (49.6.0.post20210108)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.5-cp36-cp36m-manylinux2014_x86_64.whl (20 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.5-cp36-cp36m-manylinux2014_x86_64.whl (35 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.5-cp36-cp36m-manylinux2014_x86_64.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 78.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7\n",
      "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy==2.3.5) (2.25.1)\n",
      "Collecting srsly<1.1.0,>=1.0.2\n",
      "  Downloading srsly-1.0.5-cp36-cp36m-manylinux2014_x86_64.whl (184 kB)\n",
      "\u001b[K     |████████████████████████████████| 184 kB 79.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm<5.0.0,>=4.38.0\n",
      "  Downloading tqdm-4.61.2-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 7.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting plac<1.2.0,>=0.9.6\n",
      "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.4-cp36-cp36m-manylinux2014_x86_64.whl (9.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.8 MB 62.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wasabi<1.1.0,>=0.4.0\n",
      "  Downloading wasabi-0.8.2-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy==2.3.5) (1.19.5)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.3.5) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.3.5) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.3.5) (3.4.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (1.26.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.5) (3.0.4)\n",
      "Installing collected packages: murmurhash, cymem, wasabi, tqdm, srsly, preshed, plac, catalogue, blis, thinc, spacy\n",
      "Successfully installed blis-0.7.4 catalogue-1.0.0 cymem-2.0.5 murmurhash-1.0.5 plac-1.1.3 preshed-3.0.5 spacy-2.3.5 srsly-1.0.5 thinc-7.4.5 tqdm-4.61.2 wasabi-0.8.2\n",
      "Collecting de_core_news_sm==2.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.3.0/de_core_news_sm-2.3.0.tar.gz (14.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.9 MB 6.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from de_core_news_sm==2.3.0) (2.3.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.25.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (0.7.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (0.8.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.1.3)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (7.4.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.19.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (4.61.2)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (49.6.0.post20210108)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.0.5)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.4.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2021.5.30)\n",
      "Building wheels for collected packages: de-core-news-sm\n",
      "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.3.0-py3-none-any.whl size=14907581 sha256=f573c692093fe44a9d4eb5b48ee39d8a8604f7c90cbebf1e7216bdae69fc7743\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-d1ge0xvy/wheels/fe/44/0f/7270b8ec13bc290e606a3c0f52f981915b1d09d1dfc7c79088\n",
      "Successfully built de-core-news-sm\n",
      "Installing collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-2.3.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('de_core_news_sm')\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.5.0-cp36-cp36m-manylinux2010_x86_64.whl (454.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 454.3 MB 12 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.15.2)\n",
      "Collecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
      "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 54.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wrapt~=1.12.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.7.4.3)\n",
      "Collecting keras-nightly~=2.5.0.dev\n",
      "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 57.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 290 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio~=1.34.0\n",
      "  Downloading grpcio-1.34.1-cp36-cp36m-manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 52.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 5.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py~=3.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.19.5)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tensorboard~=2.5\n",
      "  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 48.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 78.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: cached-property in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from h5py~=3.1.0->tensorflow) (1.5.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard~=2.5->tensorflow) (2.25.1)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 70.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard~=2.5->tensorflow) (49.6.0.post20210108)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard~=2.5->tensorflow) (1.0.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.32.1-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 78.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 9.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 65.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (4.7.2)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 74.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.26.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2021.5.30)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 76.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow) (3.4.0)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=dee6ffd013dc4da82e9d84b8019a963760ea957d742620b0a33274021be4133a\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "Successfully built termcolor\n",
      "Installing collected packages: pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, keras-nightly, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed absl-py-0.13.0 astunparse-1.6.3 cachetools-4.2.2 flatbuffers-1.12 gast-0.4.0 google-auth-1.32.1 google-auth-oauthlib-0.4.4 grpcio-1.34.1 keras-nightly-2.5.0.dev2021032900 keras-preprocessing-1.1.2 markdown-3.3.4 oauthlib-3.1.1 opt-einsum-3.3.0 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.5.0 tensorflow-estimator-2.5.0 termcolor-1.1.0\n",
      "Collecting keras\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (5.4.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.19.5)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (3.1.0)\n",
      "Requirement already satisfied: cached-property in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from h5py->keras) (1.5.1)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.4.3\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    already_initialized\n",
    "except NameError:\n",
    "    !python -m pip install --upgrade pip\n",
    "    !pip install spacy==2.3.5\n",
    "    !python -m spacy download de_core_news_sm\n",
    "    !pip install tensorflow\n",
    "    !pip install keras\n",
    "    already_initialized = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5e775ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n",
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64d6874",
   "metadata": {},
   "source": [
    "## Load the recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c817825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Url</th>\n",
       "      <th>Instructions</th>\n",
       "      <th>Ingredients</th>\n",
       "      <th>Day</th>\n",
       "      <th>Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.chefkoch.de/rezepte/185441079701305/</td>\n",
       "      <td>Die Eier hart kochen. Dann pellen und mit eine...</td>\n",
       "      <td>[600 g Hackfleisch, halb und halb, 800 g Sauer...</td>\n",
       "      <td>1</td>\n",
       "      <td>Gebratener Hasenrücken</td>\n",
       "      <td>2009</td>\n",
       "      <td>January</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.chefkoch.de/rezepte/2718181424631245/</td>\n",
       "      <td>Vorab folgende Bemerkung: Alle Mengen sind Cir...</td>\n",
       "      <td>[1 kg Strauchtomate(n), 1 Gemüsezwiebel(n), 1 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilz Stroganoff</td>\n",
       "      <td>2017</td>\n",
       "      <td>July</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.chefkoch.de/rezepte/46341015661368/</td>\n",
       "      <td>Die Kirschen abtropfen lassen, dabei den Saft ...</td>\n",
       "      <td>[1 Glas Kirsche(n), 1 Pck. Vanillepuddingpulve...</td>\n",
       "      <td>1</td>\n",
       "      <td>Kaninchen a la Gioff</td>\n",
       "      <td>2007</td>\n",
       "      <td>January</td>\n",
       "      <td>Monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.chefkoch.de/rezepte/51051018014178/</td>\n",
       "      <td>Den Spargel säubern, die holzigen Enden abschn...</td>\n",
       "      <td>[500 g Spargel, grüner, 300 ml Brühe oder Fond...</td>\n",
       "      <td>1</td>\n",
       "      <td>Spanisches Knoblauch-Kaninchen</td>\n",
       "      <td>2013</td>\n",
       "      <td>April</td>\n",
       "      <td>Monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.chefkoch.de/rezepte/1555771262860408/</td>\n",
       "      <td>Kohlrabi schälen und klein würfeln. Mit der Br...</td>\n",
       "      <td>[250 g Kohlrabi, 150 ml Gemüsebrühe, 150 ml Mi...</td>\n",
       "      <td>1</td>\n",
       "      <td>Gnocchi - Hack - Pfanne mit Basilikum</td>\n",
       "      <td>2017</td>\n",
       "      <td>August</td>\n",
       "      <td>Tuesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.chefkoch.de/rezepte/318661113606205/</td>\n",
       "      <td>Aus dem Mehl, der Butter, dem Ei und etwas Sal...</td>\n",
       "      <td>[Für den Mürbeteig:, 200 g Weizenmehl, 100 g B...</td>\n",
       "      <td>1</td>\n",
       "      <td>Spaghetti Siracusani</td>\n",
       "      <td>2011</td>\n",
       "      <td>August</td>\n",
       "      <td>Monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.chefkoch.de/rezepte/307991111110164/</td>\n",
       "      <td>Das Öl mit dem Rotwein, dem Essig, den Wachold...</td>\n",
       "      <td>[Für die Marinade:, 3 EL Öl, 5 EL Rotwein, 2 E...</td>\n",
       "      <td>1</td>\n",
       "      <td>Rehrücken in Salzkruste mit Waldpilzgulasch</td>\n",
       "      <td>2006</td>\n",
       "      <td>October</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.chefkoch.de/rezepte/800291183649658/</td>\n",
       "      <td>Den Reis mit 2 Tassen Wasser/Gemüsebrühe aufse...</td>\n",
       "      <td>[1 Tasse Reis, (Langkornreis), 2 Tasse/n Wasse...</td>\n",
       "      <td>1</td>\n",
       "      <td>Eier in Senfsauce</td>\n",
       "      <td>2012</td>\n",
       "      <td>March</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.chefkoch.de/rezepte/295691108953224/</td>\n",
       "      <td>Kartoffeln schälen, waschen, vierteln und in k...</td>\n",
       "      <td>[1200 g Kartoffel(n), mehlig kochend, 250 ml M...</td>\n",
       "      <td>1</td>\n",
       "      <td>Äppelufflaaf mett Woi</td>\n",
       "      <td>2006</td>\n",
       "      <td>April</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.chefkoch.de/rezepte/44381015086527/</td>\n",
       "      <td>Kartoffeln, Steckrüben, Sellerie und Zwiebel s...</td>\n",
       "      <td>[400 g Kartoffel(n), 300 g Steckrübe(n), 300 g...</td>\n",
       "      <td>1</td>\n",
       "      <td>Gefüllte Schinkenröllchen</td>\n",
       "      <td>2006</td>\n",
       "      <td>November</td>\n",
       "      <td>Wednesday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Url  \\\n",
       "0   https://www.chefkoch.de/rezepte/185441079701305/   \n",
       "1  https://www.chefkoch.de/rezepte/2718181424631245/   \n",
       "2    https://www.chefkoch.de/rezepte/46341015661368/   \n",
       "3    https://www.chefkoch.de/rezepte/51051018014178/   \n",
       "4  https://www.chefkoch.de/rezepte/1555771262860408/   \n",
       "5   https://www.chefkoch.de/rezepte/318661113606205/   \n",
       "6   https://www.chefkoch.de/rezepte/307991111110164/   \n",
       "7   https://www.chefkoch.de/rezepte/800291183649658/   \n",
       "8   https://www.chefkoch.de/rezepte/295691108953224/   \n",
       "9    https://www.chefkoch.de/rezepte/44381015086527/   \n",
       "\n",
       "                                        Instructions  \\\n",
       "0  Die Eier hart kochen. Dann pellen und mit eine...   \n",
       "1  Vorab folgende Bemerkung: Alle Mengen sind Cir...   \n",
       "2  Die Kirschen abtropfen lassen, dabei den Saft ...   \n",
       "3  Den Spargel säubern, die holzigen Enden abschn...   \n",
       "4  Kohlrabi schälen und klein würfeln. Mit der Br...   \n",
       "5  Aus dem Mehl, der Butter, dem Ei und etwas Sal...   \n",
       "6  Das Öl mit dem Rotwein, dem Essig, den Wachold...   \n",
       "7  Den Reis mit 2 Tassen Wasser/Gemüsebrühe aufse...   \n",
       "8  Kartoffeln schälen, waschen, vierteln und in k...   \n",
       "9  Kartoffeln, Steckrüben, Sellerie und Zwiebel s...   \n",
       "\n",
       "                                         Ingredients  Day  \\\n",
       "0  [600 g Hackfleisch, halb und halb, 800 g Sauer...    1   \n",
       "1  [1 kg Strauchtomate(n), 1 Gemüsezwiebel(n), 1 ...    1   \n",
       "2  [1 Glas Kirsche(n), 1 Pck. Vanillepuddingpulve...    1   \n",
       "3  [500 g Spargel, grüner, 300 ml Brühe oder Fond...    1   \n",
       "4  [250 g Kohlrabi, 150 ml Gemüsebrühe, 150 ml Mi...    1   \n",
       "5  [Für den Mürbeteig:, 200 g Weizenmehl, 100 g B...    1   \n",
       "6  [Für die Marinade:, 3 EL Öl, 5 EL Rotwein, 2 E...    1   \n",
       "7  [1 Tasse Reis, (Langkornreis), 2 Tasse/n Wasse...    1   \n",
       "8  [1200 g Kartoffel(n), mehlig kochend, 250 ml M...    1   \n",
       "9  [400 g Kartoffel(n), 300 g Steckrübe(n), 300 g...    1   \n",
       "\n",
       "                                          Name  Year     Month    Weekday  \n",
       "0                       Gebratener Hasenrücken  2009   January   Thursday  \n",
       "1                              Pilz Stroganoff  2017      July   Saturday  \n",
       "2                         Kaninchen a la Gioff  2007   January     Monday  \n",
       "3               Spanisches Knoblauch-Kaninchen  2013     April     Monday  \n",
       "4        Gnocchi - Hack - Pfanne mit Basilikum  2017    August    Tuesday  \n",
       "5                         Spaghetti Siracusani  2011    August     Monday  \n",
       "6  Rehrücken in Salzkruste mit Waldpilzgulasch  2006   October     Sunday  \n",
       "7                            Eier in Senfsauce  2012     March   Thursday  \n",
       "8                        Äppelufflaaf mett Woi  2006     April   Saturday  \n",
       "9                    Gefüllte Schinkenröllchen  2006  November  Wednesday  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(\"data/recipes.json\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89d8101",
   "metadata": {},
   "source": [
    "Define the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "676d8e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1190, 8)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df = df[11000:]\n",
    "eval_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9c6a78",
   "metadata": {},
   "source": [
    "Define the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b6cd953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11000, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[:11000]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c57a458",
   "metadata": {},
   "source": [
    "## Tokenize the texts with spacy\n",
    "**Author**: Run the spacy tokenizer on all instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90439f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 17556\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('de_core_news_sm', disable=['parser', 'tagger', 'ner'])\n",
    "tokenized = [nlp(t) for t in df.Instructions.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31caed5f",
   "metadata": {},
   "source": [
    "**Author**: And now we build a vocabulary of known tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6eefbd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 17556\n"
     ]
    }
   ],
   "source": [
    "vocab = {\"<UNK>\": 1, \"<PAD>\": 0}\n",
    "for txt in tokenized:\n",
    "    for token in txt:\n",
    "        if token.text not in vocab.keys():\n",
    "            vocab[token.text] = len(vocab)\n",
    "print(\"Number of unique tokens: {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c908f3a",
   "metadata": {},
   "source": [
    "## Create the labels\n",
    "**Author**: What is missing now, are the labels. We need to know where in the text are ingredients. We will try to bootstrap this information from the provided ingredients list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15dd4887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['600 g Hackfleisch, halb und halb',\n",
       " '800 g Sauerkraut',\n",
       " '200 g Wurst, geräucherte (Csabai Kolbász)',\n",
       " '150 g Speck, durchwachsener, geräucherter',\n",
       " '100 g Reis',\n",
       " '1 m.-große Zwiebel(n)',\n",
       " '1 Zehe/n Knoblauch',\n",
       " '2 Becher Schmand',\n",
       " '1/2TL Kümmel, ganzer',\n",
       " '2 Lorbeerblätter',\n",
       " 'Salz und Pfeffer',\n",
       " '4 Ei(er) (bei Bedarf)',\n",
       " 'Paprikapulver',\n",
       " 'etwas Wasser',\n",
       " 'Öl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredients = df.Ingredients\n",
    "ingredients[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94c6579",
   "metadata": {},
   "source": [
    "**Author**: We first clean the ingredients lists from stopwords, numbers and other stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21e4c0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _filter(token):\n",
    "    if len(token) < 2:\n",
    "        return False\n",
    "    if token.is_stop:\n",
    "        return False\n",
    "    if token.text[0].islower():\n",
    "        return False\n",
    "    if token.is_digit:\n",
    "        return False\n",
    "    if token.like_num:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def _clean(text):\n",
    "    text = text.replace(\"(\", \"\")\n",
    "    text = text.split(\"/\")[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "999a9a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rosenkohl',\n",
       " 'Schalotte',\n",
       " '2Tasse',\n",
       " 'Hühnerbrühe',\n",
       " 'Milch',\n",
       " 'EL',\n",
       " 'Crème',\n",
       " 'Speck',\n",
       " 'Kartoffelgnocchi']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean = [_clean(t.text) for i in ingredients[214] for t in nlp(i) if _filter(t) and len(_clean(t.text)) >= 2]\n",
    "clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6040859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(ingredients, tokenized_instructions):\n",
    "    labels = []\n",
    "    for ing, ti in zip(ingredients, tokenized_instructions):\n",
    "        l_i = []\n",
    "        ci = [_clean(t.text) for i in ing for t in nlp(i) if _filter(t) and len(_clean(t.text)) >= 2]\n",
    "        label = []\n",
    "        for token in ti:\n",
    "            l_i.append(any((c == token.text or c == token.text[:-1] or c[:-1] == token.text) for c in ci))\n",
    "        labels.append(l_i)\n",
    "    return labels\n",
    "\n",
    "def format_labels(labels):\n",
    "    y_seq = []\n",
    "    for label in labels:\n",
    "        y_i = []\n",
    "        for i in range(MAX_LEN):\n",
    "            try:\n",
    "                y_i.append(float(label[i]))\n",
    "            except:\n",
    "                y_i.append(0.0)\n",
    "        y_seq.append(np.array(y_i))\n",
    "    y_seq = np.array(y_seq)\n",
    "    return y_seq.reshape(y_seq.shape[0], y_seq.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3cc0d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, True, False, True, False, True, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Crème', 'Hühnerbrühe', 'Milch', 'Rosenkohl', 'Schalotten', 'Speck'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = get_labels(ingredients, tokenized)\n",
    "i=214\n",
    "print(labels[i])\n",
    "set([t.text for t, l in zip(tokenized[i], labels[i]) if l])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beea2d6",
   "metadata": {},
   "source": [
    "## Modelling with a LSTM network\n",
    "**Author**: First we have to look at the length of our recipes, to determine the length we want to pad our inputs for the network to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57b6b1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.hist([len([t for t in tokens]) for tokens in tokenized], bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bd6be0",
   "metadata": {},
   "source": [
    "**Author**: We picked a maximum length of 400 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43943ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d3364f",
   "metadata": {},
   "source": [
    "## Prepare the sequences by padding\n",
    "**Author**: Now we pad the sequences and map the words to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8419bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(texts, max_len, vocab={\"<UNK>\": 1, \"<PAD>\": 0}):\n",
    "    X = [[vocab.get(w.text, vocab[\"<UNK>\"]) for w in s] for s in texts]\n",
    "    return pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=vocab[\"<PAD>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bf9fc0",
   "metadata": {},
   "source": [
    "**Author**: Using TensorFlow backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc94ffe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([192, 193, 194, 183, 195, 196, 128, 197,   9, 198, 199, 200, 201,\n",
       "       202, 203,  60, 204, 205,   9,  13, 206,  15,  23,  98, 207, 208,\n",
       "        51, 209,  68, 202, 203,  25,   6, 195, 125, 202, 210, 211, 212,\n",
       "        33,  45, 213, 214, 100, 196,  13, 215, 216, 217,  33,   9,  68,\n",
       "       218, 219, 213, 169,  35,  82, 100, 220, 221, 202,   6, 222,  45,\n",
       "       223,  48, 224,  33,  67, 225, 100, 226,   6, 227, 228, 229, 130,\n",
       "        45,  92,  85, 230, 211, 231,   6, 232, 233, 234, 235, 145, 157,\n",
       "       236,   9, 237, 238, 104, 239, 210, 240, 157, 241,  54,   6, 109,\n",
       "       242, 243, 244, 245, 246, 187, 247,   6, 248, 183, 249, 250,  33,\n",
       "       129,  13, 251, 252, 101, 253,  33, 254,   9,  31, 255,  40, 172,\n",
       "         6,   2, 256, 257, 177, 258, 259, 260,  33,  42, 261, 262, 263,\n",
       "       131, 264, 265, 266,  33, 267,  74, 268, 269,  68, 270,   6,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_seq = prepare_sequences(tokenized, max_len=MAX_LEN, vocab=vocab)\n",
    "X_seq[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "21c001f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_seq = format_labels(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc833d79",
   "metadata": {},
   "source": [
    "# Setup the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee012bea",
   "metadata": {},
   "source": [
    "**Author**: Now we can start to setup the model. We build a simple 2-layer LSTM-based sequence tagger with tensorflow.keras."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e1e963b9",
   "metadata": {},
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Embedding(input_dim=len(vocab), mask_zero=True, output_dim=50))\n",
    "model.add(layers.SpatialDropout1D(0.2))\n",
    "model.add(layers.Bidirectional(layers.LSTM(units=64, return_sequences=True)))\n",
    "model.add(layers.SpatialDropout1D(0.2))\n",
    "model.add(layers.Bidirectional(layers.LSTM(units=64, return_sequences=True)))\n",
    "model.add(layers.TimeDistributed(layers.Dense(1, activation='sigmoid')))\n",
    "\n",
    "model.compile(optimizer=tf.optimizers.Adam(0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd06821",
   "metadata": {},
   "source": [
    "**Author**: And now we fit it."
   ]
  },
  {
   "cell_type": "raw",
   "id": "300d458e",
   "metadata": {},
   "source": [
    "history = model.fit(X_seq, y_seq, epochs=10, batch_size=256, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "202ad8c4",
   "metadata": {},
   "source": [
    "from datetime import datetime\n",
    "now = datetime.today().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "model_name = f'benchmark_model_{now}_loss-{history.history[\"loss\"][-1]:.2f}'\n",
    "model.save(f'./model/{model_name}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "67e86d5d",
   "metadata": {},
   "source": [
    "from s3util import S3Util\n",
    "s3util = S3Util()\n",
    "s3util.save_models_to_s3()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e69b704",
   "metadata": {},
   "source": [
    "from importlib import reload\n",
    "import s3util\n",
    "reload(s3util)\n",
    "model_name = 'benchmark_model_2021-03-13_14-57-29_loss-0.02'\n",
    "s3u = s3util.S3Util()\n",
    "s3u.get_model_from_s3(model_name)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd2da1c6",
   "metadata": {},
   "source": [
    "s3u.unpack(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b84736",
   "metadata": {},
   "source": [
    "**Yeray**: The code above was originally used to train the model. Since it takes quite some time, I saved the trained model which I then used in subsequent runs of this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0823c2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('model/benchmark_model_2021-03-13_14-57-29_loss-0.02')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b482b654",
   "metadata": {},
   "source": [
    "## Analyse the predictions of the model\n",
    "**Author**: Now that the model is trained, we can look at some predictions on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a51f5fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 25s 913ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_seq, verbose=1, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520239f3",
   "metadata": {},
   "source": [
    "**Yeray**: Define a predictions vector with True or False if the word a that particular location is an ingredient or not:**Yeray**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e5135013",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3343\n",
    "pred_i = y_pred[i] > 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750b9660",
   "metadata": {},
   "source": [
    "**Yeray**: Original text as a list of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ded02c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Kohlrabi schälen, waschen und in Stifte schneiden. Brühe und Milch ankochen, Kohlrabi dazugeben, aufkochen lassen und 10 Minuten kochen. Dann herausnehmen und abtropfen lassen, die Brühe aufheben.Butter erhitzen, das Mehl darin anschwitzen, mit Kohlrabibrühe ablöschen und aufkochen lassen. Mit den Gewürzen abschmecken. Kohlrabi wieder dazugeben.Hähnchenbrust schnetzeln, kräftig anbraten und würzen. Das Fleisch in eine Auflaufform geben, die Speckwürfel darüber verteilen. Mit Käse bestreuen. Nun das Gemüse darüber schichten und alles bei 180 °C ca. 25 Minuten überbacken.Tipp:Man kann auch gut gekochte, in Würfel geschnittene Kartoffeln unter die Kohlrabi mischen. Ebenso kann man auch Kohlrabi und Möhren für den Auflauf nehmen. Schmeckt auch sehr lecker!"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b062d27b",
   "metadata": {},
   "source": [
    "**Yeray**: The list of tokens in the text with a prediction greater than 0.05:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "337f7a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Mehl', 'Butter', 'Kohlrabi', 'Kartoffeln', 'Speckwürfel', 'Möhren', 'Milch', 'Käse', 'Gemüse', 'Brühe'}\n"
     ]
    }
   ],
   "source": [
    "ingreds = [t.text for t, p in zip(tokenized[i], pred_i) if p]\n",
    "print(set(ingreds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab2293c",
   "metadata": {},
   "source": [
    "**Yeray**: The list of tokens in the text that were originally labeled as ingredients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30344b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Butter', 'Kohlrabi', 'Käse', 'Mehl', 'Milch'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingreds = [t.text for t, p in zip(tokenized[i], y_seq[i]) if p]\n",
    "set(ingreds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315a5d4c",
   "metadata": {},
   "source": [
    "**Yeray**: The actual list of ingredients used for labelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7d2112b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['500 g Kohlrabi',\n",
       " '1/4Liter Hühnerbrühe',\n",
       " '1/4Liter Milch',\n",
       " '1 EL Butter',\n",
       " '30 g Mehl',\n",
       " '300 g Hähnchenbrustfilet(s)',\n",
       " 'Salz und Pfeffer',\n",
       " 'Muskat',\n",
       " '50 g Käse, gerieben',\n",
       " '50 g Speck, gewürfelt']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredients[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb94ab1",
   "metadata": {},
   "source": [
    "**Author**: This looks very good! Our model seems to be able to identify the ingredients better than our training labels. So we now use the produced labels for fine-tuning the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0a456a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = []\n",
    "for pred_i, ti in zip(y_pred, tokenized):\n",
    "    l_i = []\n",
    "    ci = [t.text for t, p in zip(tokenized[i], pred_i > 0.05) if p]\n",
    "    label = []\n",
    "    for token in ti:\n",
    "        l_i.append(any((c == token.text or c == token.text[:-1] or c[:-1] == token.text) for c in ci))\n",
    "    new_labels.append(l_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1589b641",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_seq_new = format_labels(new_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb95349f",
   "metadata": {},
   "source": [
    "**Author**: We fit the network again for one epoch with the new labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41ecc59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 73s 2s/step - loss: 0.2585 - accuracy: 0.7276 - val_loss: 0.1793 - val_accuracy: 0.7834\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_seq, y_seq_new, epochs=1, batch_size=256, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833dbdc4",
   "metadata": {},
   "source": [
    "# Look at test data\n",
    "**Author**: Now we can look at the test data we put aside in the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f564aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['500 g Rosenkohl',\n",
       " '400 g Kartoffel(n)',\n",
       " '3 Stange/n Staudensellerie',\n",
       " '2 Zwiebel(n), gewürfelte',\n",
       " '4 EL Olivenöl',\n",
       " '1 TL Currypulver',\n",
       " '1/2TL Kurkuma',\n",
       " '300 ml Gemüsebrühe',\n",
       " '3 EL Tomatenmark',\n",
       " '50 g Rosinen',\n",
       " '1/2TL Chilipulver oder frischer Chili',\n",
       " '1/2Bund Petersilie',\n",
       " '2 Banane(n)',\n",
       " '1/2 Zitrone(n), der Saft davon',\n",
       " '150 g Schlagsahne',\n",
       " '1 EL Zucker, braun bei Bedarf']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_ingredients = eval_df.Ingredients.values\n",
    "eval_ingredients[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0987662a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rosenkohl putzen, die Röschen halbieren. Kartoffeln schälen und würfeln. Staudensellerie putzen und in 1 cm große Stücke schneiden. Zwiebeln schälen und würfeln.Öl in einer Pfanne erhitzen, Chili und Zwiebeln darin anbraten. Mit Currypulver und Kurkuma bestreuen. Tomatenmark zugeben und kurz mit anschwitzen (je nach Geschmack 1 EL braunen Zucker zugeben). Rosenkohl, Kartoffeln und Sellerie zugeben. Kurz andünsten und die Brühe angießen. Rosinen zugeben und mit geschlossenem Deckel ca. 15 Minuten köcheln.Die Bananen schälen und in Scheiben schneiden. Die Bananenscheiben und Zitronensaft zugeben und noch einmal 10 Minuten köcheln lassen. Die Sahne nicht ganz steif schlagen und unter das Rosenkohl-Curry heben. Mit Petersilie bestreut servieren."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_tokenized = [nlp(t) for t in eval_df.Instructions.values]\n",
    "eval_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4c9f27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3285,  205,   33,   98,  564,  869,    6,  520,   32,    9,   30,\n",
       "          6, 3669,  205,    9,   13,  110,  333,  264,  206,   15,    6,\n",
       "        585,   32,    9,   30,    6,   92,   13,  233,  420,  459,   33,\n",
       "       2271,    9,  585,  422,  341,    6,  109, 1236,    9, 1680,  548,\n",
       "          6,  612,  568,    9,   62,   10,  703,   23,  361,  199,  200,\n",
       "        110,  277, 3670,  295,  568,   25,    6, 3285,   33,  520,    9,\n",
       "        560,  568,    6,  691, 1132,    9,   98,  220,  570,    6, 1725,\n",
       "        568,    9,   10, 1254,  148,  288,  571,   79,  290,    6,    2,\n",
       "       3353,   32,    9,   13,   14,   15,    6,    2, 3671,    9, 1122,\n",
       "        568,    9,  124,  302,  344,   79,  290,   54,    6,    2,  426,\n",
       "         51,   52,  311,  312,    9,   80,   45, 3672, 1083,    6,  109,\n",
       "        380,  574,  321,    6,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_seq_test = prepare_sequences(eval_tokenized, max_len=MAX_LEN, vocab=vocab)\n",
    "X_seq_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3b462ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_labels = get_labels(eval_ingredients, eval_tokenized)\n",
    "\n",
    "y_seq_test = format_labels(eval_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94887509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 362ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = model.predict(X_seq_test, verbose=1, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "024eecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(i):\n",
    "    pred_i = y_pred_test[i] > 0.05\n",
    "    print(eval_tokenized[i])\n",
    "    print()\n",
    "    print(eval_ingredients[i])\n",
    "    print()\n",
    "    ingreds = [t.text for t, p in zip(eval_tokenized[i], pred_i) if p]\n",
    "    print(set(ingreds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4d93c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Den Quark durch ein Sieb in eine tiefe Schüssel streichen.Das Mehl, den Zucker, Salz, Vanillezucker und das rohe Ei/er gut verrühren.Diese Masse auf einem mit Mehl bestreuten Backbrett zu einer dicken Wurst rollen und in 10 gleichgroße Scheiben schneiden. In heißer Butter von beiden Seiten goldbraun braten.Die fertigen Tworoshniki werden mit Puderzucker bestreut oder warm mit saurer Sahne oder Obstsirup zu Tisch gebracht.\n",
      "\n",
      "['500 g Quark, sehr trockenen', '80 g Mehl', '2 EL Zucker', '1 Pck. Vanillezucker', 'Salz', '1 Ei(er), evt. 2', '4 EL Butter oder Margarine', 'Puderzucker', '125 ml Sirup (Obstsirup) oder saure Sahne', 'Mehl für die Arbeitsfläche']\n",
      "\n",
      "{'Backbrett', 'Mehl', ',', 'verrühren', 'heißer', 'dicken', 'von', 'tiefe', '.', 'auf', 'ein', 'In', 'Puderzucker', 'schneiden', 'Sieb', 'Den', 'einem', 'braten', 'Scheiben', 'streichen', 'Tisch', 'Obstsirup', 'durch', 'rollen', 'werden', 'Ei', 'oder', 'eine', 'goldbraun', 'Quark', 'bestreuten', 'rohe', 'beiden', 'Wurst', 'gleichgroße', 'bestreut', 'fertigen', 'saurer', 'Schüssel', 'den', 'einer', 'mit', '10', 'warm', 'Salz', 'und', 'gut', 'Masse', 'Das', 'Butter', '/', 'Diese', 'zu', 'das', 'Vanillezucker', 'Sahne', 'Seiten', 'gebracht', 'er', 'Die', 'in', 'Tworoshniki', 'Zucker'}\n"
     ]
    }
   ],
   "source": [
    "pred(893)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "645b7c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spargel putzen und bissfest garen. Herausnehmen, abschrecken und warm stellen.Fisch mit Salz und Pfeffer würzen. Öl in einer Pfanne erhitzen und den Lachs darin 3-4 Min. je Seite braten. Butter schmelzen, Mandeln hinzufügen und leicht bräunen. Schale der Limette mit einem Zestenreißer abziehen, den Saft auspressen, beides in die Butter geben. Mit Salz und Pfeffer würzen.Spargel abtropfen lassen, mit Lachs anrichten und mit Mandelbutter beträufeln.Dazu passen Salzkartoffeln.\n",
      "\n",
      "['500 g Spargel, weißer', '500 g Spargel, grüner', 'Salz und Pfeffer', '4 Scheibe/n Lachsfilet(s) (à ca. 200g)', '2 EL Öl', '100 g Butter', '30 g Mandel(n) in Blättchen', '1 Limette(n), unbehandelt']\n",
      "\n",
      "{'bissfest', 'Pfeffer', ',', 'Pfanne', 'Saft', 'Fisch', 'hinzufügen', 'würzen', '.', 'abziehen', 'abschrecken', 'einem', 'braten', 'schmelzen', 'je', 'Mandelbutter', 'Herausnehmen', 'beides', 'Schale', 'Lachs', 'stellen', 'Dazu', 'Öl', 'Spargel', 'Mandeln', 'Seite', 'Mit', 'abtropfen', 'lassen', 'Min', '3', 'putzen', 'den', 'einer', 'geben', 'mit', 'warm', 'Salz', 'bräunen', 'und', 'der', '-', 'Butter', 'leicht', 'beträufeln', 'Salzkartoffeln', 'anrichten', 'erhitzen', 'auspressen', 'garen', '4', 'in', 'Limette', 'Zestenreißer', 'darin', 'die'}\n"
     ]
    }
   ],
   "source": [
    "pred(26)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f0ac35",
   "metadata": {},
   "source": [
    "**Author**: This looks quite good! We build a quite strong model to identify ingredients in recipes. I hope you learned something and had some fun. You can try to improve the model by manual labeling or adding labels from a dictionary of ingredients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491c6596",
   "metadata": {},
   "source": [
    "## Evaluating Model\n",
    "\n",
    "**Yeray**: This part of the code was not in the original article. Here I try to calculate different metrics like precision, recall and f1-score in order to compare the performance of this model with my own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b4d0aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y, pred):\n",
    "    true_positives  = np.sum(y[pred == 1] == 1)\n",
    "    false_positives = np.sum(y[pred == 1] == 0)\n",
    "\n",
    "    return true_positives / (true_positives + false_positives)\n",
    "\n",
    "def recall(y, pred):\n",
    "    true_positives  = np.sum(y[pred == 1] == 1)\n",
    "    false_negatives = np.sum(y[pred == 0] == 1)\n",
    "\n",
    "    return true_positives / (true_positives + false_negatives)\n",
    "\n",
    "def f1_score(y, pred):\n",
    "    p = precision(y, pred)\n",
    "    r = recall(y, pred)\n",
    "    \n",
    "    return 2 * p * r / (p + r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3e8cf6",
   "metadata": {},
   "source": [
    "**Yeray**: Calculate precision, recall and f1-score on the **Training Set** used to train this benchmark model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf4bd736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      " - precision: 4.43%\n",
      " - recall: 97.82%\n",
      " - f1_score: 8.47%\n"
     ]
    }
   ],
   "source": [
    "y = y_seq.reshape(-1)\n",
    "pred = y_pred.reshape(-1) > 0.05\n",
    "\n",
    "print('Training Set')\n",
    "print(f' - precision: {precision(y, pred):.2%}')\n",
    "print(f' - recall: {recall(y, pred):.2%}')\n",
    "print(f' - f1_score: {f1_score(y, pred):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0595786d",
   "metadata": {},
   "source": [
    "**Yeray**: Calculate precision, recall and f1-score on the **Test Set** of this benchmark model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c8debd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set\n",
      " - precision: 3.09%\n",
      " - recall: 100.00%\n",
      " - f1_score: 6.00%\n"
     ]
    }
   ],
   "source": [
    "eval_y = y_seq_test.reshape(-1)\n",
    "eval_pred = y_pred_test.reshape(-1) > 0.05\n",
    "\n",
    "print('Test Set')\n",
    "print(f' - precision: {precision(eval_y, eval_pred):.2%}')\n",
    "print(f' - recall: {recall(eval_y, eval_pred):.2%}')\n",
    "print(f' - f1_score: {f1_score(eval_y, eval_pred):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baef2dc",
   "metadata": {},
   "source": [
    "**Yeray**: Finally calculate precision, recall and f1-score on **my own data** used to train my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4e972ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "My own data\n",
      " - precision: 0.26%\n",
      " - recall: 99.83%\n",
      " - f1_score: 0.53%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "csv_file = 'data/data.csv'\n",
    "my_df = pd.read_csv(csv_file)\n",
    "my_ingredients = [[ingredient_name] for ingredient_name in my_df.name.values]\n",
    "my_tokenized = [nlp(t) for t in my_df.ingredient.values]\n",
    "my_X = prepare_sequences(my_tokenized, max_len=MAX_LEN, vocab=vocab)\n",
    "my_labels = get_labels(my_ingredients, my_tokenized)\n",
    "my_y = format_labels(my_labels)\n",
    "my_pred = model.predict(my_X, verbose=1, batch_size=1024) > 0.05\n",
    "\n",
    "print('My own data')\n",
    "print(f' - precision: {precision(my_y.reshape(-1), my_pred.reshape(-1)):.2%}')\n",
    "print(f' - recall: {recall(my_y.reshape(-1), my_pred.reshape(-1)):.2%}')\n",
    "print(f' - f1_score: {f1_score(my_y.reshape(-1), my_pred.reshape(-1)):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7a4d6c",
   "metadata": {},
   "source": [
    "**Yeray**: Here one can see that the model optimized for recall and does quite well on that metric on my own data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
