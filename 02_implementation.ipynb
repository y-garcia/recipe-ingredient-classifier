{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    already_initialized\n",
    "except NameError:\n",
    "    !python -m pip install --upgrade pip\n",
    "    !pip install spacy==2.3.5\n",
    "    !pip install spacy-lookups-data\n",
    "    !python -m spacy download de_core_news_sm\n",
    "    already_initialized = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy.lang.de import German\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "import random\n",
    "import pprint\n",
    "import string\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'data/data.csv'\n",
    "ingredients_df = pd.read_csv(csv_file)\n",
    "\n",
    "print(f\"Total number of rows: {len(ingredients_df.index)}\")\n",
    "\n",
    "# print out the first few rows of data info\n",
    "ingredients_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprinter = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp(title, obj, newline=True):\n",
    "    print(f'{title}:')\n",
    "    pprinter.pprint(obj)\n",
    "    if(newline):\n",
    "        print()\n",
    "\n",
    "def split_words(series):\n",
    "    unique = set()\n",
    "    for item in series:\n",
    "        cleaned = re.sub(r'['+re.escape(string.punctuation)+']', ' ', item)\n",
    "        cleaned = re.sub(r' +', ' ', cleaned)\n",
    "        words = cleaned.split()\n",
    "        for word in words:\n",
    "            unique.add(word)\n",
    "    return list(unique)\n",
    "    \n",
    "def generate_patterns(model, series, label):\n",
    "    entity_patterns = []\n",
    "    matcher_patterns = []\n",
    "    \n",
    "    for item in series:\n",
    "        pattern = item.lower() if isinstance(item, str) else f'{item:.2f}'\n",
    "        entity_patterns.append({'label': label, 'pattern': pattern})\n",
    "        matcher_patterns.append(model.make_doc(pattern))\n",
    "    \n",
    "    return (entity_patterns, matcher_patterns)\n",
    "\n",
    "def generate_data(model, data):\n",
    "    RESULT = []\n",
    "    for doc in model.pipe(data):\n",
    "        entities = []\n",
    "        for match_id, start, end in matcher(doc):\n",
    "            span = doc[start:end]\n",
    "            label = model.vocab.strings[match_id]\n",
    "            entity = (span.start_char, span.end_char, label)\n",
    "            entities.append(entity)\n",
    "\n",
    "        training_example = (doc.text, {'entities': entities})\n",
    "        RESULT.append(training_example)\n",
    "    return RESULT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generate patterns\n",
    "We are going to generate patterns we can use for automatically labeling our training data using spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = German()\n",
    "\n",
    "UNITS = ingredients_df.unit.unique()\n",
    "\n",
    "(unit_entity_patterns, unit_matcher_patterns) = generate_patterns(nlp, UNITS, 'UNIT')\n",
    "\n",
    "pp('Unit entity examples:', unit_entity_patterns[0:10])\n",
    "pp('Unit pattern examples', unit_matcher_patterns[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUANTITIES = ingredients_df.quantity.unique()\n",
    "\n",
    "(quantity_entity_patterns, quantity_matcher_patterns) = generate_patterns(nlp, QUANTITIES, 'QUANTITY')\n",
    "\n",
    "pp('Quantity entity examples:', quantity_entity_patterns[0:10])\n",
    "pp('Quantity pattern examples', quantity_matcher_patterns[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES = ingredients_df.name.unique()\n",
    "\n",
    "UNIQUE_WORDS = split_words(NAMES)\n",
    "\n",
    "(name_entity_patterns, name_matcher_patterns) = generate_patterns(nlp, UNIQUE_WORDS, 'NAME')\n",
    "\n",
    "pp('Name entity examples:', name_entity_patterns[0:10])\n",
    "pp('Name pattern examples', name_matcher_patterns[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define a Metric\n",
    "\n",
    "Different types of metrics can be used:\n",
    "1. accuracy:\n",
    "  - `(true positives + true negatives) / total`\n",
    "  - In other words: correctly predicted / total\n",
    "  - Question accuracy answers: How many selected items were correctly categorized?\n",
    "2. recall:\n",
    "  - `true positives / (true positives + false negatives)`\n",
    "  - In other words: words correctly identified as entities / all words that are entities\n",
    "  - Question recall answers: How many relevant items are selected?\n",
    "3. precision:\n",
    "  - `true positives / (true positives + false positives)`\n",
    "  - In other words: words correctly identified as entities / words correctly and incorrectly identified as entities\n",
    "  - Question precision answers: How many selected items are relevant?\n",
    "\n",
    "`Accuracy` can be misleading if we have imbalanced data.\n",
    "\n",
    "`Recall` is in our case more important than `precision`, since we want to catch all ingredients,\n",
    "and we don't care if some non-ingredients are marked as such, we can just delete them later in the app with a\n",
    "single click of a button. On the other hand not recognizing ingredients would force us to type them manually.\n",
    "\n",
    "We still don't want to have too many non-ingredients in our ingredient's list though, so we could to use the `F1-Score`\n",
    "which combines `recall` and `precision` as a metric for our problem.\n",
    "\n",
    "Another approach could be to use precision as a **satisficing metric** and recall as an **optimizing metric**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create Training Data\n",
    "Now we are going to loop through all the texts in the ingredient column and mark each part (quantity, unit and name) as different entities using the patterns we generated in step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of examples - train: 343 - dev: 115 - test: 115\n",
    "TRAIN_DEV_SET, TEST_SET = train_test_split(ingredients_df, test_size=0.2)\n",
    "TRAIN_SET, DEV_SET = train_test_split(TRAIN_DEV_SET, test_size=0.25)\n",
    "\n",
    "TRAIN_DEV_SET = None\n",
    "\n",
    "total = len(ingredients_df)\n",
    "train_percent = len(TRAIN_SET) / total\n",
    "dev_percent = len(DEV_SET) / total\n",
    "test_percent = len(TEST_SET) / total\n",
    "\n",
    "print(f'train-dev-test split: {train_percent:.0%} - {dev_percent:.0%} - {test_percent:.0%}')\n",
    "print(f'train set: {len(TRAIN_SET)} examples') # 343\n",
    "print(f'  dev set: {len(DEV_SET)} examples')   # 115\n",
    "print(f' test set: {len(TEST_SET)} examples')  # 115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n",
    "matcher.add('QUANTITY', None, *quantity_matcher_patterns)\n",
    "matcher.add('UNIT', None, *unit_matcher_patterns)\n",
    "matcher.add('NAME', None, *name_matcher_patterns)\n",
    "\n",
    "TRAIN_DATA = generate_data(nlp, TRAIN_SET.ingredient)\n",
    "DEV_DATA = generate_data(nlp, DEV_SET.ingredient)\n",
    "TEST_DATA = generate_data(nlp, TEST_SET.ingredient)\n",
    "\n",
    "pp('TRAIN_DATA', TRAIN_DATA[0:5])\n",
    "pp('DEV_DATA', DEV_DATA[0:5])\n",
    "pp('TEST_DATA', TEST_DATA[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('de')\n",
    "ner = nlp.create_pipe('ner')\n",
    "nlp.add_pipe(ner)\n",
    "ner.add_label('QUANTITY')\n",
    "ner.add_label('UNIT')\n",
    "ner.add_label('NAME')\n",
    "\n",
    "# TODO 2. improve training model https://v2.spacy.io/usage/training\n",
    "\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "hyper = {'iterations': 10, 'minibatch_size': 2, 'dropout': 0.2}\n",
    "loss_history = []\n",
    "for iteration in range(hyper['iterations']):\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    \n",
    "    losses = {}\n",
    "    \n",
    "    batches = spacy.util.minibatch(TRAIN_DATA, size=hyper['minibatch_size'])\n",
    "    for batch in batches:\n",
    "        texts = []\n",
    "        annotations = []\n",
    "        for text, entity_offsets in batch:\n",
    "            doc = nlp.make_doc(text)\n",
    "            gold = GoldParse(doc, entities=entity_offsets['entities'])\n",
    "            spacy.gold.biluo_tags_from_offsets(doc, entity_offsets['entities'])\n",
    "            texts.append(doc)\n",
    "            annotations.append(gold)\n",
    "        nlp.update(texts, annotations, losses=losses, sgd=optimizer, drop=hyper['dropout'])\n",
    "        \n",
    "    loss_history.append(losses['ner'])\n",
    "    print(f'iteration {iteration} - loss: {losses}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.today().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "model_name = f'model_{now}_loss-{loss_history[-1]:.2f}'\n",
    "Path(f'./model').mkdir(parents=True, exist_ok=True)\n",
    "nlp.to_disk(f'./model/{model_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model from Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_load = model_name;\n",
    "\n",
    "nlp = spacy.load(f'model/{model_to_load}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_to_try = 10\n",
    "for i in range(examples_to_try):\n",
    "    random.shuffle(TEST_DATA)\n",
    "    text, _ = TEST_DATA[i]\n",
    "    doc = nlp(text)\n",
    "    print(text, '\\n\\t==>', [(ent.label_, ent.text) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    scorer = Scorer()\n",
    "    for text, annotations in data:\n",
    "        doc = model.make_doc(text)\n",
    "        gold = GoldParse(doc, entities=annotations['entities'])\n",
    "        prediction = model(text)\n",
    "        scorer.score(prediction, gold)\n",
    "    return scorer.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp('Scores on TRAIN_DATA', evaluate(nlp, TRAIN_DATA))\n",
    "pp('Scores on DEV_DATA', evaluate(nlp, DEV_DATA))\n",
    "pp('Scores on TEST_DATA', evaluate(nlp, TEST_DATA))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
