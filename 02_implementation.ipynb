{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade pip\n",
    "!pip install spacy==2.3.5\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'data/data.csv'\n",
    "ingredients_df = pd.read_csv(csv_file)\n",
    "\n",
    "print(f\"Total number of rows: {len(ingredients_df.index)}\")\n",
    "\n",
    "# print out the first few rows of data info\n",
    "ingredients_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprinter = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.de import German\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = German()\n",
    "\n",
    "def pp(title, obj, newline=True):\n",
    "    print(f'{title}:')\n",
    "    pprinter.pprint(obj)\n",
    "    if(newline):\n",
    "        print()\n",
    "\n",
    "def generate_patterns(nlp, series, label):\n",
    "    entity_patterns = []\n",
    "    matcher_patterns = []\n",
    "    \n",
    "    for item in series:\n",
    "        pattern = item.lower() if isinstance(item, str) else f'{item:.2f}'\n",
    "        entity_patterns.append({'label': label, 'pattern': pattern})\n",
    "        matcher_patterns.append(nlp.make_doc(pattern))\n",
    "    \n",
    "    return (entity_patterns, matcher_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generate patterns\n",
    "We are going to generate patterns we can use for automatically labeling our training data using spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNITS = ingredients_df.unit.unique()\n",
    "\n",
    "(unit_entity_patterns, unit_matcher_patterns) = generate_patterns(nlp, UNITS, 'UNIT')\n",
    "\n",
    "pp('Unit entity examples:', unit_entity_patterns[0:10])\n",
    "pp('Unit pattern examples', unit_matcher_patterns[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUANTITIES = ingredients_df.quantity.unique()\n",
    "\n",
    "(quantity_entity_patterns, quantity_matcher_patterns) = generate_patterns(nlp, QUANTITIES, 'QUANTITY')\n",
    "\n",
    "pp('Quantity entity examples:', quantity_entity_patterns[0:10])\n",
    "pp('Quantity pattern examples', quantity_matcher_patterns[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES = ingredients_df.name.unique()\n",
    "\n",
    "(name_entity_patterns, name_matcher_patterns) = generate_patterns(nlp, NAMES, 'NAME')\n",
    "\n",
    "pp('Name entity examples:', name_entity_patterns[0:10])\n",
    "pp('Name pattern examples', name_matcher_patterns[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define a Metric\n",
    "\n",
    "Different types of metrics can be used:\n",
    "1. accuracy:\n",
    "  - `(true positives + true negatives) / total`\n",
    "  - In other words: correctly predicted / total\n",
    "  - Question accuracy answers: How many selected items were correctly categorized?\n",
    "2. recall:\n",
    "  - `true positives / (true positives + false negatives)`\n",
    "  - In other words: words correctly identified as entities / all words that are entities\n",
    "  - Question recall answers: How many relevant items are selected?\n",
    "3. precision:\n",
    "  - `true positives / (true positives + false positives)`\n",
    "  - In other words: words correctly identified as entities / words correctly and incorrectly identified as entities\n",
    "  - Question precision answers: How many selected items are relevant?\n",
    "\n",
    "`Accuracy` can be misleading if we have imbalanced data.\n",
    "\n",
    "`Recall` is in our case more important than `precision`, since we want to catch all ingredients,\n",
    "and we don't care if some non-ingredients are marked as such, we can just delete them later in the app with a\n",
    "single click of a button. On the other hand not recognizing ingredients would force us to type them manually.\n",
    "\n",
    "We still don't want to have too many non-ingredients in our ingredient's list though, so we could to use the `F1-Score`\n",
    "which combines `recall` and `precision` as a metric for our problem.\n",
    "\n",
    "Another approach could be to use precision as a **satisficing metric** and recall as an **optimizing metric**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create Training Data\n",
    "Now we are going to loop through all the texts in the ingredient column and mark each part (quantity, unit and name) as different entities using the patterns we generated in step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Number of examples - train: 343 - dev: 115 - test: 115\n",
    "TRAIN_DEV_SET, TEST_SET = train_test_split(ingredients_df, test_size=0.2)\n",
    "TRAIN_SET, DEV_SET = train_test_split(TRAIN_DEV_SET, test_size=0.25)\n",
    "\n",
    "total = len(ingredients_df)\n",
    "train_percent = len(TRAIN_SET) / total\n",
    "dev_percent = len(DEV_SET) / total\n",
    "test_percent = len(TEST_SET) / total\n",
    "\n",
    "print(f'train-dev-test split: {train_percent:.0%} - {dev_percent:.0%} - {test_percent:.0%}')\n",
    "print(f'train set: {len(TRAIN_SET)} examples') # 343\n",
    "print(f'  dev set: {len(DEV_SET)} examples')   # 115\n",
    "print(f' test set: {len(TEST_SET)} examples')  # 115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n",
    "matcher.add('QUANTITY', None, *quantity_matcher_patterns)\n",
    "matcher.add('UNIT', None, *unit_matcher_patterns)\n",
    "matcher.add('NAME', None, *name_matcher_patterns)\n",
    "\n",
    "TRAIN_DATA = []\n",
    "\n",
    "for doc in nlp.pipe(ingredients_df.ingredient):\n",
    "    entities = []\n",
    "    for match_id, start, end in matcher(doc):\n",
    "        span = doc[start:end]\n",
    "        label = nlp.vocab.strings[match_id]\n",
    "        entity = (span.start_char, span.end_char, label)\n",
    "        entities.append(entity)\n",
    "        \n",
    "    training_example = (doc.text, {'entities': entities})\n",
    "    TRAIN_DATA.append(training_example)\n",
    "    \n",
    "    print(training_example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
